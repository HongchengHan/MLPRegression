{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1724, 3)\n",
      "[[ 0.     30.    ]\n",
      " [ 0.     30.7864]\n",
      " [ 0.     31.8199]\n",
      " ...\n",
      " [ 0.5282 57.3585]\n",
      " [ 0.5531 57.9181]\n",
      " [ 0.5779 58.475 ]]\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv('./data_train_all_0824.csv')\n",
    "data_array = np.array(data_df)\n",
    "x_train_all = data_array[:, :3]\n",
    "y_train_all = data_array[:, 3:]\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, test_size=0.2, shuffle=False, random_state=1)\n",
    "x_test = np.array(pd.read_csv('./data_eval_v_0.7.csv'))[:, :3]\n",
    "y_test = np.array(pd.read_csv('./data_eval_v_0.7.csv'))[:, 3:]\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(x_train)\n",
    "scaler_y = MinMaxScaler()\n",
    "scaler_y.fit(y_train)\n",
    "\n",
    "x_train_copy = x_train.copy()\n",
    "x_valid_copy = x_valid.copy()\n",
    "x_test_copy = x_test.copy()\n",
    "\n",
    "y_train_copy = y_train.copy()\n",
    "y_valid_copy = y_valid.copy()\n",
    "y_test_copy = y_test.copy()\n",
    "\n",
    "x_train = scaler_x.transform(x_train)\n",
    "x_valid = scaler_x.transform(x_valid)\n",
    "x_test = scaler_x.transform(x_test)\n",
    "x_train_all = scaler_x.transform(x_train_all)\n",
    "\n",
    "# x_train = scaler_x.transform(x_train).reshape(-1, 1, 3)\n",
    "# x_valid = scaler_x.transform(x_valid).reshape(-1, 1, 3)\n",
    "# x_test = scaler_x.transform(x_test).reshape(-1, 1, 3)\n",
    "# x_train_all = scaler_x.transform(x_train_all).reshape(-1, 1, 3)\n",
    "\n",
    "y_train = scaler_y.transform(y_train)\n",
    "y_valid = scaler_y.transform(y_valid)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(scaler_y.inverse_transform(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLPRegressor(solver='adam', alpha=1e-3, hidden_layer_sizes=(32, 16), activation='logistic', learning_rate='adaptive', power_t=0.5, \n",
    "#     learning_rate_init=0.005, max_iter=2000, tol=1e-4, early_stopping=False, validation_fraction=0.1, random_state=1)\n",
    "\n",
    "# model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def mlp_regressor():\n",
    "    model = Sequential()\n",
    "    # model.add(Input(shape=(1, 3)))\n",
    "    model.add(Input(shape=(3,)))\n",
    "    # model.add(LSTM(32, return_sequences=False))\n",
    "    model.add(Dense(16, activation='sigmoid'))\n",
    "    model.add(Dense(16, activation='sigmoid'))\n",
    "    # model.add(Dense(8, activation='sigmoid'))\n",
    "    # model.add(Dense(8, activation='sigmoid'))\n",
    "    model.add(Dense(2, activation=None))\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.01), metrics=['mse'])   \n",
    "    return model\n",
    "\n",
    "# def mlp_regressor():\n",
    "#     inputs = Input(shape=(3,))\n",
    "#     inputs = inputs[:, :2]\n",
    "#     x_1 = Dense(8, activation='sigmoid')(inputs)\n",
    "#     x_2 = Dense(16, activation='sigmoid')(inputs)\n",
    "#     x = Dense(8, activation='sigmoid')(x_2) + x_1\n",
    "#     outputs = Dense(2, activation=None)(x)\n",
    "#     model = Model(inputs=[inputs], outputs=outputs)\n",
    "#     model.compile(loss='mse', optimizer=Adam(lr=0.01), metrics=['mse'])   \n",
    "#     return model\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_mse', min_delta=1e-5, patience=100),\n",
    "             ModelCheckpoint(\"./checkpoints/mlp.hdf5\", monitor='val_mse',\n",
    "                             mode='min', verbose=0, save_best_only=True)]\n",
    "\n",
    "model = mlp_regressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 0.1668 - mse: 0.1668 - val_loss: 0.1064 - val_mse: 0.1064\n",
      "Epoch 2/1000\n",
      "108/108 [==============================] - 0s 543us/step - loss: 0.0865 - mse: 0.0865 - val_loss: 0.0985 - val_mse: 0.0985\n",
      "Epoch 3/1000\n",
      "108/108 [==============================] - 0s 481us/step - loss: 0.0562 - mse: 0.0562 - val_loss: 0.0571 - val_mse: 0.0571\n",
      "Epoch 4/1000\n",
      "108/108 [==============================] - 0s 501us/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.0403 - val_mse: 0.0403\n",
      "Epoch 5/1000\n",
      "108/108 [==============================] - 0s 395us/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0503 - val_mse: 0.0503\n",
      "Epoch 6/1000\n",
      "108/108 [==============================] - 0s 523us/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0365 - val_mse: 0.0365\n",
      "Epoch 7/1000\n",
      "108/108 [==============================] - 0s 392us/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0401 - val_mse: 0.0401\n",
      "Epoch 8/1000\n",
      "108/108 [==============================] - 0s 490us/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0298 - val_mse: 0.0298\n",
      "Epoch 9/1000\n",
      "108/108 [==============================] - 0s 472us/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0229 - val_mse: 0.0229\n",
      "Epoch 10/1000\n",
      "108/108 [==============================] - 0s 464us/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0207 - val_mse: 0.0207\n",
      "Epoch 11/1000\n",
      "108/108 [==============================] - 0s 800us/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0184 - val_mse: 0.0184\n",
      "Epoch 12/1000\n",
      "108/108 [==============================] - 0s 541us/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 13/1000\n",
      "108/108 [==============================] - 0s 473us/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 14/1000\n",
      "108/108 [==============================] - 0s 420us/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0155 - val_mse: 0.0155\n",
      "Epoch 15/1000\n",
      "108/108 [==============================] - 0s 452us/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0141 - val_mse: 0.0141\n",
      "Epoch 16/1000\n",
      "108/108 [==============================] - 0s 495us/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 17/1000\n",
      "108/108 [==============================] - 0s 436us/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 18/1000\n",
      "108/108 [==============================] - 0s 403us/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 19/1000\n",
      "108/108 [==============================] - 0s 443us/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 20/1000\n",
      "108/108 [==============================] - 0s 379us/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 21/1000\n",
      "108/108 [==============================] - 0s 399us/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0104 - val_mse: 0.0104\n",
      "Epoch 22/1000\n",
      "108/108 [==============================] - 0s 407us/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 23/1000\n",
      "108/108 [==============================] - 0s 455us/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0092 - val_mse: 0.0092\n",
      "Epoch 24/1000\n",
      "108/108 [==============================] - 0s 390us/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 25/1000\n",
      "108/108 [==============================] - 0s 518us/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0087 - val_mse: 0.0087\n",
      "Epoch 26/1000\n",
      "108/108 [==============================] - 0s 511us/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0090 - val_mse: 0.0090\n",
      "Epoch 27/1000\n",
      "108/108 [==============================] - 0s 443us/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0086 - val_mse: 0.0086\n",
      "Epoch 28/1000\n",
      "108/108 [==============================] - 0s 459us/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0095 - val_mse: 0.0095\n",
      "Epoch 29/1000\n",
      "108/108 [==============================] - 0s 383us/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0089 - val_mse: 0.0089\n",
      "Epoch 30/1000\n",
      "108/108 [==============================] - 0s 413us/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0072 - val_mse: 0.0072\n",
      "Epoch 31/1000\n",
      "108/108 [==============================] - 0s 471us/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0063 - val_mse: 0.0063\n",
      "Epoch 32/1000\n",
      "108/108 [==============================] - 0s 439us/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 33/1000\n",
      "108/108 [==============================] - 0s 473us/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 34/1000\n",
      "108/108 [==============================] - 0s 402us/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0058 - val_mse: 0.0058\n",
      "Epoch 35/1000\n",
      "108/108 [==============================] - 0s 407us/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 36/1000\n",
      "108/108 [==============================] - 0s 400us/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 37/1000\n",
      "108/108 [==============================] - 0s 448us/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 38/1000\n",
      "108/108 [==============================] - 0s 371us/step - loss: 9.5950e-04 - mse: 9.5950e-04 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 39/1000\n",
      "108/108 [==============================] - 0s 509us/step - loss: 9.5297e-04 - mse: 9.5297e-04 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 40/1000\n",
      "108/108 [==============================] - 0s 375us/step - loss: 8.6386e-04 - mse: 8.6386e-04 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 41/1000\n",
      "108/108 [==============================] - 0s 470us/step - loss: 7.5400e-04 - mse: 7.5400e-04 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 42/1000\n",
      "108/108 [==============================] - 0s 367us/step - loss: 7.8424e-04 - mse: 7.8424e-04 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 43/1000\n",
      "108/108 [==============================] - 0s 621us/step - loss: 7.2994e-04 - mse: 7.2994e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 44/1000\n",
      "108/108 [==============================] - 0s 568us/step - loss: 6.7459e-04 - mse: 6.7459e-04 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 45/1000\n",
      "108/108 [==============================] - 0s 574us/step - loss: 6.3596e-04 - mse: 6.3596e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 46/1000\n",
      "108/108 [==============================] - 0s 371us/step - loss: 6.4494e-04 - mse: 6.4494e-04 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 47/1000\n",
      "108/108 [==============================] - 0s 407us/step - loss: 5.7241e-04 - mse: 5.7241e-04 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 48/1000\n",
      "108/108 [==============================] - 0s 379us/step - loss: 6.1564e-04 - mse: 6.1564e-04 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 49/1000\n",
      "108/108 [==============================] - 0s 397us/step - loss: 5.6411e-04 - mse: 5.6411e-04 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 50/1000\n",
      "108/108 [==============================] - 0s 410us/step - loss: 5.4986e-04 - mse: 5.4986e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 51/1000\n",
      "108/108 [==============================] - 0s 345us/step - loss: 5.3427e-04 - mse: 5.3427e-04 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 52/1000\n",
      "108/108 [==============================] - 0s 376us/step - loss: 4.7050e-04 - mse: 4.7050e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 53/1000\n",
      "108/108 [==============================] - 0s 388us/step - loss: 4.7142e-04 - mse: 4.7142e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 54/1000\n",
      "108/108 [==============================] - 0s 392us/step - loss: 4.5951e-04 - mse: 4.5951e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 55/1000\n",
      "108/108 [==============================] - 0s 395us/step - loss: 4.8027e-04 - mse: 4.8027e-04 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 56/1000\n",
      "108/108 [==============================] - 0s 407us/step - loss: 4.5948e-04 - mse: 4.5948e-04 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 57/1000\n",
      "108/108 [==============================] - 0s 460us/step - loss: 3.9085e-04 - mse: 3.9085e-04 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 58/1000\n",
      "108/108 [==============================] - 0s 487us/step - loss: 4.3409e-04 - mse: 4.3409e-04 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 59/1000\n",
      "108/108 [==============================] - 0s 364us/step - loss: 3.9221e-04 - mse: 3.9221e-04 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 60/1000\n",
      "108/108 [==============================] - 0s 597us/step - loss: 3.9671e-04 - mse: 3.9671e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 61/1000\n",
      "108/108 [==============================] - 0s 385us/step - loss: 3.5689e-04 - mse: 3.5689e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 62/1000\n",
      "108/108 [==============================] - 0s 408us/step - loss: 3.7369e-04 - mse: 3.7369e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 63/1000\n",
      "108/108 [==============================] - 0s 448us/step - loss: 4.2335e-04 - mse: 4.2335e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 64/1000\n",
      "108/108 [==============================] - 0s 535us/step - loss: 3.7632e-04 - mse: 3.7632e-04 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 65/1000\n",
      "108/108 [==============================] - 0s 433us/step - loss: 3.2863e-04 - mse: 3.2863e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 66/1000\n",
      "108/108 [==============================] - 0s 379us/step - loss: 3.2314e-04 - mse: 3.2314e-04 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 67/1000\n",
      "108/108 [==============================] - 0s 374us/step - loss: 3.7777e-04 - mse: 3.7777e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 68/1000\n",
      "108/108 [==============================] - 0s 402us/step - loss: 3.2874e-04 - mse: 3.2874e-04 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 69/1000\n",
      "108/108 [==============================] - 0s 385us/step - loss: 3.6274e-04 - mse: 3.6274e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 70/1000\n",
      "108/108 [==============================] - 0s 448us/step - loss: 3.0027e-04 - mse: 3.0027e-04 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 71/1000\n",
      "108/108 [==============================] - 0s 463us/step - loss: 3.1278e-04 - mse: 3.1278e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 72/1000\n",
      "108/108 [==============================] - 0s 446us/step - loss: 2.8562e-04 - mse: 2.8562e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 73/1000\n",
      "108/108 [==============================] - 0s 499us/step - loss: 2.9509e-04 - mse: 2.9509e-04 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 74/1000\n",
      "108/108 [==============================] - 0s 529us/step - loss: 3.0446e-04 - mse: 3.0446e-04 - val_loss: 0.0038 - val_mse: 0.0038\n",
      "Epoch 75/1000\n",
      "108/108 [==============================] - 0s 533us/step - loss: 3.1628e-04 - mse: 3.1628e-04 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 76/1000\n",
      "108/108 [==============================] - 0s 454us/step - loss: 2.9381e-04 - mse: 2.9381e-04 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 77/1000\n",
      "108/108 [==============================] - 0s 421us/step - loss: 2.6954e-04 - mse: 2.6954e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 78/1000\n",
      "108/108 [==============================] - 0s 410us/step - loss: 2.5597e-04 - mse: 2.5597e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 79/1000\n",
      "108/108 [==============================] - 0s 398us/step - loss: 2.6874e-04 - mse: 2.6874e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 80/1000\n",
      "108/108 [==============================] - 0s 388us/step - loss: 2.4271e-04 - mse: 2.4271e-04 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 81/1000\n",
      "108/108 [==============================] - 0s 402us/step - loss: 2.4612e-04 - mse: 2.4612e-04 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 82/1000\n",
      "108/108 [==============================] - 0s 496us/step - loss: 2.3769e-04 - mse: 2.3769e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 83/1000\n",
      "108/108 [==============================] - 0s 450us/step - loss: 2.2573e-04 - mse: 2.2573e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 84/1000\n",
      "108/108 [==============================] - 0s 475us/step - loss: 2.0949e-04 - mse: 2.0949e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 85/1000\n",
      "108/108 [==============================] - 0s 549us/step - loss: 2.2460e-04 - mse: 2.2460e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 86/1000\n",
      "108/108 [==============================] - 0s 495us/step - loss: 2.2630e-04 - mse: 2.2630e-04 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 87/1000\n",
      "108/108 [==============================] - 0s 502us/step - loss: 1.9865e-04 - mse: 1.9865e-04 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 88/1000\n",
      "108/108 [==============================] - 0s 605us/step - loss: 1.9834e-04 - mse: 1.9834e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 89/1000\n",
      "108/108 [==============================] - 0s 507us/step - loss: 1.8058e-04 - mse: 1.8058e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 90/1000\n",
      "108/108 [==============================] - 0s 391us/step - loss: 1.7255e-04 - mse: 1.7255e-04 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 91/1000\n",
      "108/108 [==============================] - 0s 376us/step - loss: 1.7141e-04 - mse: 1.7141e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 92/1000\n",
      "108/108 [==============================] - 0s 382us/step - loss: 1.6871e-04 - mse: 1.6871e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 93/1000\n",
      "108/108 [==============================] - 0s 444us/step - loss: 1.7102e-04 - mse: 1.7102e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 94/1000\n",
      "108/108 [==============================] - 0s 382us/step - loss: 1.6635e-04 - mse: 1.6635e-04 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 95/1000\n",
      "108/108 [==============================] - 0s 395us/step - loss: 1.4728e-04 - mse: 1.4728e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 96/1000\n",
      "108/108 [==============================] - 0s 410us/step - loss: 1.4563e-04 - mse: 1.4563e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 97/1000\n",
      "108/108 [==============================] - 0s 397us/step - loss: 1.6621e-04 - mse: 1.6621e-04 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 98/1000\n",
      "108/108 [==============================] - 0s 492us/step - loss: 1.5696e-04 - mse: 1.5696e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 99/1000\n",
      "108/108 [==============================] - 0s 487us/step - loss: 1.4658e-04 - mse: 1.4658e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 100/1000\n",
      "108/108 [==============================] - 0s 426us/step - loss: 1.3167e-04 - mse: 1.3167e-04 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 101/1000\n",
      "108/108 [==============================] - 0s 475us/step - loss: 1.6328e-04 - mse: 1.6328e-04 - val_loss: 0.0016 - val_mse: 0.0016\n",
      "Epoch 102/1000\n",
      "108/108 [==============================] - 0s 402us/step - loss: 1.2121e-04 - mse: 1.2121e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 103/1000\n",
      "108/108 [==============================] - 0s 427us/step - loss: 1.2566e-04 - mse: 1.2566e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 104/1000\n",
      "108/108 [==============================] - 0s 390us/step - loss: 1.2197e-04 - mse: 1.2197e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 105/1000\n",
      "108/108 [==============================] - 0s 391us/step - loss: 1.1640e-04 - mse: 1.1640e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 106/1000\n",
      "108/108 [==============================] - 0s 388us/step - loss: 1.1794e-04 - mse: 1.1794e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 107/1000\n",
      "108/108 [==============================] - 0s 455us/step - loss: 1.3072e-04 - mse: 1.3072e-04 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 108/1000\n",
      "108/108 [==============================] - 0s 484us/step - loss: 1.1168e-04 - mse: 1.1168e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 109/1000\n",
      "108/108 [==============================] - 0s 575us/step - loss: 1.2238e-04 - mse: 1.2238e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 110/1000\n",
      "108/108 [==============================] - 0s 479us/step - loss: 9.9782e-05 - mse: 9.9782e-05 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 111/1000\n",
      "108/108 [==============================] - 0s 484us/step - loss: 9.6985e-05 - mse: 9.6985e-05 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 112/1000\n",
      "108/108 [==============================] - 0s 409us/step - loss: 1.1380e-04 - mse: 1.1380e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 113/1000\n",
      "108/108 [==============================] - 0s 482us/step - loss: 1.0898e-04 - mse: 1.0898e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 114/1000\n",
      "108/108 [==============================] - 0s 435us/step - loss: 1.0607e-04 - mse: 1.0607e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 115/1000\n",
      "108/108 [==============================] - 0s 443us/step - loss: 8.7135e-05 - mse: 8.7135e-05 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 116/1000\n",
      "108/108 [==============================] - 0s 498us/step - loss: 1.0547e-04 - mse: 1.0547e-04 - val_loss: 0.0015 - val_mse: 0.0015\n",
      "Epoch 117/1000\n",
      "108/108 [==============================] - 0s 434us/step - loss: 9.0774e-05 - mse: 9.0774e-05 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 118/1000\n",
      "108/108 [==============================] - 0s 380us/step - loss: 9.2480e-05 - mse: 9.2480e-05 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 119/1000\n",
      "108/108 [==============================] - 0s 455us/step - loss: 9.5077e-05 - mse: 9.5077e-05 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 120/1000\n",
      "108/108 [==============================] - 0s 406us/step - loss: 1.2063e-04 - mse: 1.2063e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 121/1000\n",
      "108/108 [==============================] - 0s 447us/step - loss: 9.1990e-05 - mse: 9.1990e-05 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 122/1000\n",
      "108/108 [==============================] - 0s 467us/step - loss: 8.3636e-05 - mse: 8.3636e-05 - val_loss: 9.8901e-04 - val_mse: 9.8901e-04\n",
      "Epoch 123/1000\n",
      "108/108 [==============================] - 0s 438us/step - loss: 1.2199e-04 - mse: 1.2199e-04 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 124/1000\n",
      "108/108 [==============================] - 0s 473us/step - loss: 7.4179e-05 - mse: 7.4179e-05 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 125/1000\n",
      "108/108 [==============================] - 0s 361us/step - loss: 8.1688e-05 - mse: 8.1688e-05 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 126/1000\n",
      "108/108 [==============================] - 0s 457us/step - loss: 9.9214e-05 - mse: 9.9214e-05 - val_loss: 0.0010 - val_mse: 0.0010\n",
      "Epoch 127/1000\n",
      "108/108 [==============================] - 0s 352us/step - loss: 7.8237e-05 - mse: 7.8237e-05 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 128/1000\n",
      "108/108 [==============================] - 0s 435us/step - loss: 8.1880e-05 - mse: 8.1880e-05 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 129/1000\n",
      "108/108 [==============================] - 0s 397us/step - loss: 8.7436e-05 - mse: 8.7436e-05 - val_loss: 0.0012 - val_mse: 0.0012\n",
      "Epoch 130/1000\n",
      "108/108 [==============================] - 0s 517us/step - loss: 9.3242e-05 - mse: 9.3242e-05 - val_loss: 9.6455e-04 - val_mse: 9.6455e-04\n",
      "Epoch 131/1000\n",
      "108/108 [==============================] - 0s 445us/step - loss: 9.9417e-05 - mse: 9.9417e-05 - val_loss: 8.0931e-04 - val_mse: 8.0931e-04\n",
      "Epoch 132/1000\n",
      "108/108 [==============================] - 0s 456us/step - loss: 7.4123e-05 - mse: 7.4123e-05 - val_loss: 9.9644e-04 - val_mse: 9.9644e-04\n",
      "Epoch 133/1000\n",
      "108/108 [==============================] - 0s 481us/step - loss: 7.6832e-05 - mse: 7.6832e-05 - val_loss: 8.6497e-04 - val_mse: 8.6497e-04\n",
      "Epoch 134/1000\n",
      "108/108 [==============================] - 0s 437us/step - loss: 7.4453e-05 - mse: 7.4453e-05 - val_loss: 8.0609e-04 - val_mse: 8.0609e-04\n",
      "Epoch 135/1000\n",
      "108/108 [==============================] - 0s 452us/step - loss: 7.9184e-05 - mse: 7.9184e-05 - val_loss: 8.2046e-04 - val_mse: 8.2046e-04\n",
      "Epoch 136/1000\n",
      "108/108 [==============================] - 0s 399us/step - loss: 7.7747e-05 - mse: 7.7747e-05 - val_loss: 8.5414e-04 - val_mse: 8.5414e-04\n",
      "Epoch 137/1000\n",
      "108/108 [==============================] - 0s 426us/step - loss: 7.6720e-05 - mse: 7.6720e-05 - val_loss: 8.3060e-04 - val_mse: 8.3060e-04\n",
      "Epoch 138/1000\n",
      "108/108 [==============================] - 0s 487us/step - loss: 6.6278e-05 - mse: 6.6278e-05 - val_loss: 7.1019e-04 - val_mse: 7.1019e-04\n",
      "Epoch 139/1000\n",
      "108/108 [==============================] - 0s 408us/step - loss: 9.6509e-05 - mse: 9.6509e-05 - val_loss: 9.6828e-04 - val_mse: 9.6828e-04\n",
      "Epoch 140/1000\n",
      "108/108 [==============================] - 0s 439us/step - loss: 7.3498e-05 - mse: 7.3498e-05 - val_loss: 7.8366e-04 - val_mse: 7.8366e-04\n",
      "Epoch 141/1000\n",
      "108/108 [==============================] - 0s 403us/step - loss: 8.1655e-05 - mse: 8.1655e-05 - val_loss: 7.2405e-04 - val_mse: 7.2405e-04\n",
      "Epoch 142/1000\n",
      "108/108 [==============================] - 0s 378us/step - loss: 7.1862e-05 - mse: 7.1862e-05 - val_loss: 8.2439e-04 - val_mse: 8.2439e-04\n",
      "Epoch 143/1000\n",
      "108/108 [==============================] - 0s 486us/step - loss: 6.9635e-05 - mse: 6.9635e-05 - val_loss: 7.4192e-04 - val_mse: 7.4192e-04\n",
      "Epoch 144/1000\n",
      "108/108 [==============================] - 0s 512us/step - loss: 7.5432e-05 - mse: 7.5432e-05 - val_loss: 7.8034e-04 - val_mse: 7.8034e-04\n",
      "Epoch 145/1000\n",
      "108/108 [==============================] - 0s 559us/step - loss: 6.7019e-05 - mse: 6.7019e-05 - val_loss: 6.1103e-04 - val_mse: 6.1103e-04\n",
      "Epoch 146/1000\n",
      "108/108 [==============================] - 0s 395us/step - loss: 6.6142e-05 - mse: 6.6142e-05 - val_loss: 7.2371e-04 - val_mse: 7.2371e-04\n",
      "Epoch 147/1000\n",
      "108/108 [==============================] - 0s 437us/step - loss: 7.7061e-05 - mse: 7.7061e-05 - val_loss: 8.4287e-04 - val_mse: 8.4287e-04\n",
      "Epoch 148/1000\n",
      "108/108 [==============================] - 0s 377us/step - loss: 8.8962e-05 - mse: 8.8962e-05 - val_loss: 6.5921e-04 - val_mse: 6.5921e-04\n",
      "Epoch 149/1000\n",
      "108/108 [==============================] - 0s 454us/step - loss: 6.3524e-05 - mse: 6.3524e-05 - val_loss: 6.5040e-04 - val_mse: 6.5040e-04\n",
      "Epoch 150/1000\n",
      "108/108 [==============================] - 0s 416us/step - loss: 6.5214e-05 - mse: 6.5214e-05 - val_loss: 6.9673e-04 - val_mse: 6.9673e-04\n",
      "Epoch 151/1000\n",
      "108/108 [==============================] - 0s 406us/step - loss: 8.3059e-05 - mse: 8.3059e-05 - val_loss: 8.0943e-04 - val_mse: 8.0943e-04\n",
      "Epoch 152/1000\n",
      "108/108 [==============================] - 0s 520us/step - loss: 5.6729e-05 - mse: 5.6729e-05 - val_loss: 6.1032e-04 - val_mse: 6.1032e-04\n",
      "Epoch 153/1000\n",
      "108/108 [==============================] - 0s 482us/step - loss: 1.2413e-04 - mse: 1.2413e-04 - val_loss: 6.6017e-04 - val_mse: 6.6017e-04\n",
      "Epoch 154/1000\n",
      "108/108 [==============================] - 0s 425us/step - loss: 6.3970e-05 - mse: 6.3970e-05 - val_loss: 4.6197e-04 - val_mse: 4.6197e-04\n",
      "Epoch 155/1000\n",
      "108/108 [==============================] - 0s 401us/step - loss: 6.3923e-05 - mse: 6.3923e-05 - val_loss: 7.8999e-04 - val_mse: 7.8999e-04\n",
      "Epoch 156/1000\n",
      "108/108 [==============================] - 0s 435us/step - loss: 8.0829e-05 - mse: 8.0829e-05 - val_loss: 6.6335e-04 - val_mse: 6.6335e-04\n",
      "Epoch 157/1000\n",
      "108/108 [==============================] - 0s 371us/step - loss: 9.1795e-05 - mse: 9.1795e-05 - val_loss: 6.4981e-04 - val_mse: 6.4981e-04\n",
      "Epoch 158/1000\n",
      "108/108 [==============================] - 0s 434us/step - loss: 8.0283e-05 - mse: 8.0283e-05 - val_loss: 6.5779e-04 - val_mse: 6.5779e-04\n",
      "Epoch 159/1000\n",
      "108/108 [==============================] - 0s 370us/step - loss: 7.7166e-05 - mse: 7.7166e-05 - val_loss: 5.0102e-04 - val_mse: 5.0102e-04\n",
      "Epoch 160/1000\n",
      "108/108 [==============================] - 0s 429us/step - loss: 6.4272e-05 - mse: 6.4272e-05 - val_loss: 5.4709e-04 - val_mse: 5.4709e-04\n",
      "Epoch 161/1000\n",
      "108/108 [==============================] - 0s 396us/step - loss: 6.9561e-05 - mse: 6.9561e-05 - val_loss: 4.7682e-04 - val_mse: 4.7682e-04\n",
      "Epoch 162/1000\n",
      "108/108 [==============================] - 0s 434us/step - loss: 7.3968e-05 - mse: 7.3968e-05 - val_loss: 5.2369e-04 - val_mse: 5.2369e-04\n",
      "Epoch 163/1000\n",
      "108/108 [==============================] - 0s 428us/step - loss: 5.7914e-05 - mse: 5.7914e-05 - val_loss: 8.1013e-04 - val_mse: 8.1013e-04\n",
      "Epoch 164/1000\n",
      "108/108 [==============================] - 0s 428us/step - loss: 7.6183e-05 - mse: 7.6183e-05 - val_loss: 5.8591e-04 - val_mse: 5.8591e-04\n",
      "Epoch 165/1000\n",
      "108/108 [==============================] - 0s 406us/step - loss: 6.9007e-05 - mse: 6.9007e-05 - val_loss: 5.0811e-04 - val_mse: 5.0811e-04\n",
      "Epoch 166/1000\n",
      "108/108 [==============================] - 0s 399us/step - loss: 5.9495e-05 - mse: 5.9495e-05 - val_loss: 5.6140e-04 - val_mse: 5.6140e-04\n",
      "Epoch 167/1000\n",
      "108/108 [==============================] - 0s 399us/step - loss: 6.4196e-05 - mse: 6.4196e-05 - val_loss: 7.6732e-04 - val_mse: 7.6732e-04\n",
      "Epoch 168/1000\n",
      "108/108 [==============================] - 0s 447us/step - loss: 8.9271e-05 - mse: 8.9271e-05 - val_loss: 5.7114e-04 - val_mse: 5.7114e-04\n",
      "Epoch 169/1000\n",
      "108/108 [==============================] - 0s 395us/step - loss: 8.0585e-05 - mse: 8.0585e-05 - val_loss: 6.8827e-04 - val_mse: 6.8827e-04\n",
      "Epoch 170/1000\n",
      "108/108 [==============================] - 0s 409us/step - loss: 7.1889e-05 - mse: 7.1889e-05 - val_loss: 7.1333e-04 - val_mse: 7.1333e-04\n",
      "Epoch 171/1000\n",
      "108/108 [==============================] - 0s 381us/step - loss: 5.5100e-05 - mse: 5.5100e-05 - val_loss: 6.2951e-04 - val_mse: 6.2951e-04\n",
      "Epoch 172/1000\n",
      "108/108 [==============================] - 0s 436us/step - loss: 5.8615e-05 - mse: 5.8615e-05 - val_loss: 6.1435e-04 - val_mse: 6.1435e-04\n",
      "Epoch 173/1000\n",
      "108/108 [==============================] - 0s 458us/step - loss: 6.9945e-05 - mse: 6.9945e-05 - val_loss: 2.8057e-04 - val_mse: 2.8057e-04\n",
      "Epoch 174/1000\n",
      "108/108 [==============================] - 0s 472us/step - loss: 6.5558e-05 - mse: 6.5558e-05 - val_loss: 7.9689e-04 - val_mse: 7.9689e-04\n",
      "Epoch 175/1000\n",
      "108/108 [==============================] - 0s 865us/step - loss: 6.6083e-05 - mse: 6.6083e-05 - val_loss: 5.4865e-04 - val_mse: 5.4865e-04\n",
      "Epoch 176/1000\n",
      "108/108 [==============================] - 0s 497us/step - loss: 5.3780e-05 - mse: 5.3780e-05 - val_loss: 6.0890e-04 - val_mse: 6.0890e-04\n",
      "Epoch 177/1000\n",
      "108/108 [==============================] - 0s 405us/step - loss: 5.6173e-05 - mse: 5.6173e-05 - val_loss: 5.3935e-04 - val_mse: 5.3935e-04\n",
      "Epoch 178/1000\n",
      "108/108 [==============================] - 0s 360us/step - loss: 6.3109e-05 - mse: 6.3109e-05 - val_loss: 4.9516e-04 - val_mse: 4.9516e-04\n",
      "Epoch 179/1000\n",
      "108/108 [==============================] - 0s 479us/step - loss: 4.9558e-05 - mse: 4.9558e-05 - val_loss: 6.4876e-04 - val_mse: 6.4876e-04\n",
      "Epoch 180/1000\n",
      "108/108 [==============================] - 0s 384us/step - loss: 4.9048e-05 - mse: 4.9048e-05 - val_loss: 6.4933e-04 - val_mse: 6.4933e-04\n",
      "Epoch 181/1000\n",
      "108/108 [==============================] - 0s 440us/step - loss: 5.8701e-05 - mse: 5.8701e-05 - val_loss: 4.8756e-04 - val_mse: 4.8756e-04\n",
      "Epoch 182/1000\n",
      "108/108 [==============================] - 0s 381us/step - loss: 7.6370e-05 - mse: 7.6370e-05 - val_loss: 5.3994e-04 - val_mse: 5.3994e-04\n",
      "Epoch 183/1000\n",
      "108/108 [==============================] - 0s 458us/step - loss: 6.4170e-05 - mse: 6.4170e-05 - val_loss: 3.6684e-04 - val_mse: 3.6684e-04\n",
      "Epoch 184/1000\n",
      "108/108 [==============================] - 0s 377us/step - loss: 7.3393e-05 - mse: 7.3393e-05 - val_loss: 5.8849e-04 - val_mse: 5.8849e-04\n",
      "Epoch 185/1000\n",
      "108/108 [==============================] - 0s 424us/step - loss: 7.3073e-05 - mse: 7.3073e-05 - val_loss: 8.2194e-04 - val_mse: 8.2194e-04\n",
      "Epoch 186/1000\n",
      "108/108 [==============================] - 0s 408us/step - loss: 6.3400e-05 - mse: 6.3400e-05 - val_loss: 5.8631e-04 - val_mse: 5.8631e-04\n",
      "Epoch 187/1000\n",
      "108/108 [==============================] - 0s 362us/step - loss: 5.5799e-05 - mse: 5.5799e-05 - val_loss: 4.2470e-04 - val_mse: 4.2470e-04\n",
      "Epoch 188/1000\n",
      "108/108 [==============================] - 0s 460us/step - loss: 5.2583e-05 - mse: 5.2583e-05 - val_loss: 3.2766e-04 - val_mse: 3.2766e-04\n",
      "Epoch 189/1000\n",
      "108/108 [==============================] - 0s 439us/step - loss: 5.4073e-05 - mse: 5.4073e-05 - val_loss: 5.0070e-04 - val_mse: 5.0070e-04\n",
      "Epoch 190/1000\n",
      "108/108 [==============================] - 0s 419us/step - loss: 5.8788e-05 - mse: 5.8788e-05 - val_loss: 5.0966e-04 - val_mse: 5.0966e-04\n",
      "Epoch 191/1000\n",
      "108/108 [==============================] - 0s 371us/step - loss: 6.1916e-05 - mse: 6.1916e-05 - val_loss: 6.6914e-04 - val_mse: 6.6914e-04\n",
      "Epoch 192/1000\n",
      "108/108 [==============================] - 0s 432us/step - loss: 5.1659e-05 - mse: 5.1659e-05 - val_loss: 6.2765e-04 - val_mse: 6.2765e-04\n",
      "Epoch 193/1000\n",
      "108/108 [==============================] - 0s 373us/step - loss: 5.9499e-05 - mse: 5.9499e-05 - val_loss: 4.3144e-04 - val_mse: 4.3144e-04\n",
      "Epoch 194/1000\n",
      "108/108 [==============================] - 0s 474us/step - loss: 5.3237e-05 - mse: 5.3237e-05 - val_loss: 4.1853e-04 - val_mse: 4.1853e-04\n",
      "Epoch 195/1000\n",
      "108/108 [==============================] - 0s 365us/step - loss: 8.6755e-05 - mse: 8.6755e-05 - val_loss: 3.9803e-04 - val_mse: 3.9803e-04\n",
      "Epoch 196/1000\n",
      "108/108 [==============================] - 0s 425us/step - loss: 5.5030e-05 - mse: 5.5030e-05 - val_loss: 5.2458e-04 - val_mse: 5.2458e-04\n",
      "Epoch 197/1000\n",
      "108/108 [==============================] - 0s 447us/step - loss: 7.1658e-05 - mse: 7.1658e-05 - val_loss: 5.1612e-04 - val_mse: 5.1612e-04\n",
      "Epoch 198/1000\n",
      "108/108 [==============================] - 0s 409us/step - loss: 5.7755e-05 - mse: 5.7755e-05 - val_loss: 3.6192e-04 - val_mse: 3.6192e-04\n",
      "Epoch 199/1000\n",
      "108/108 [==============================] - 0s 434us/step - loss: 7.5125e-05 - mse: 7.5125e-05 - val_loss: 6.1111e-04 - val_mse: 6.1111e-04\n",
      "Epoch 200/1000\n",
      "108/108 [==============================] - 0s 407us/step - loss: 6.3873e-05 - mse: 6.3873e-05 - val_loss: 3.0875e-04 - val_mse: 3.0875e-04\n",
      "Epoch 201/1000\n",
      "108/108 [==============================] - 0s 450us/step - loss: 5.2289e-05 - mse: 5.2289e-05 - val_loss: 6.4418e-04 - val_mse: 6.4418e-04\n",
      "Epoch 202/1000\n",
      "108/108 [==============================] - 0s 403us/step - loss: 4.2814e-05 - mse: 4.2814e-05 - val_loss: 4.3593e-04 - val_mse: 4.3593e-04\n",
      "Epoch 203/1000\n",
      "108/108 [==============================] - 0s 362us/step - loss: 5.2833e-05 - mse: 5.2833e-05 - val_loss: 4.6636e-04 - val_mse: 4.6636e-04\n",
      "Epoch 204/1000\n",
      "108/108 [==============================] - 0s 443us/step - loss: 5.0583e-05 - mse: 5.0583e-05 - val_loss: 5.2164e-04 - val_mse: 5.2164e-04\n",
      "Epoch 205/1000\n",
      "108/108 [==============================] - 0s 397us/step - loss: 5.4726e-05 - mse: 5.4726e-05 - val_loss: 4.6641e-04 - val_mse: 4.6641e-04\n",
      "Epoch 206/1000\n",
      "108/108 [==============================] - 0s 413us/step - loss: 5.7043e-05 - mse: 5.7043e-05 - val_loss: 4.8690e-04 - val_mse: 4.8690e-04\n",
      "Epoch 207/1000\n",
      "108/108 [==============================] - 0s 417us/step - loss: 5.1333e-05 - mse: 5.1333e-05 - val_loss: 6.1749e-04 - val_mse: 6.1749e-04\n",
      "Epoch 208/1000\n",
      "108/108 [==============================] - 0s 409us/step - loss: 6.3167e-05 - mse: 6.3167e-05 - val_loss: 8.0678e-04 - val_mse: 8.0678e-04\n",
      "Epoch 209/1000\n",
      "108/108 [==============================] - 0s 453us/step - loss: 5.3621e-05 - mse: 5.3621e-05 - val_loss: 7.6690e-04 - val_mse: 7.6690e-04\n",
      "Epoch 210/1000\n",
      "108/108 [==============================] - 0s 363us/step - loss: 8.8564e-05 - mse: 8.8564e-05 - val_loss: 8.4228e-04 - val_mse: 8.4228e-04\n",
      "Epoch 211/1000\n",
      "108/108 [==============================] - 0s 376us/step - loss: 6.2241e-05 - mse: 6.2241e-05 - val_loss: 4.3918e-04 - val_mse: 4.3918e-04\n",
      "Epoch 212/1000\n",
      "108/108 [==============================] - 0s 416us/step - loss: 4.8051e-05 - mse: 4.8051e-05 - val_loss: 3.4888e-04 - val_mse: 3.4888e-04\n",
      "Epoch 213/1000\n",
      "108/108 [==============================] - 0s 425us/step - loss: 5.5728e-05 - mse: 5.5728e-05 - val_loss: 4.8463e-04 - val_mse: 4.8463e-04\n",
      "Epoch 214/1000\n",
      "108/108 [==============================] - 0s 416us/step - loss: 4.8145e-05 - mse: 4.8145e-05 - val_loss: 5.6164e-04 - val_mse: 5.6164e-04\n",
      "Epoch 215/1000\n",
      "108/108 [==============================] - 0s 420us/step - loss: 5.6994e-05 - mse: 5.6994e-05 - val_loss: 4.4548e-04 - val_mse: 4.4548e-04\n",
      "Epoch 216/1000\n",
      "108/108 [==============================] - 0s 382us/step - loss: 5.9635e-05 - mse: 5.9635e-05 - val_loss: 5.2150e-04 - val_mse: 5.2150e-04\n",
      "Epoch 217/1000\n",
      "108/108 [==============================] - 0s 406us/step - loss: 5.7264e-05 - mse: 5.7264e-05 - val_loss: 5.9121e-04 - val_mse: 5.9121e-04\n",
      "Epoch 218/1000\n",
      "108/108 [==============================] - 0s 474us/step - loss: 6.9671e-05 - mse: 6.9671e-05 - val_loss: 2.2077e-04 - val_mse: 2.2077e-04\n",
      "Epoch 219/1000\n",
      "108/108 [==============================] - 0s 392us/step - loss: 5.3727e-05 - mse: 5.3727e-05 - val_loss: 4.3382e-04 - val_mse: 4.3382e-04\n",
      "Epoch 220/1000\n",
      "108/108 [==============================] - 0s 457us/step - loss: 6.6468e-05 - mse: 6.6468e-05 - val_loss: 3.5990e-04 - val_mse: 3.5990e-04\n",
      "Epoch 221/1000\n",
      "108/108 [==============================] - 0s 374us/step - loss: 4.8199e-05 - mse: 4.8199e-05 - val_loss: 2.6725e-04 - val_mse: 2.6725e-04\n",
      "Epoch 222/1000\n",
      "108/108 [==============================] - 0s 455us/step - loss: 4.8860e-05 - mse: 4.8860e-05 - val_loss: 3.0444e-04 - val_mse: 3.0444e-04\n",
      "Epoch 223/1000\n",
      "108/108 [==============================] - 0s 373us/step - loss: 5.7593e-05 - mse: 5.7593e-05 - val_loss: 3.9574e-04 - val_mse: 3.9574e-04\n",
      "Epoch 224/1000\n",
      "108/108 [==============================] - 0s 468us/step - loss: 4.9589e-05 - mse: 4.9589e-05 - val_loss: 2.9412e-04 - val_mse: 2.9412e-04\n",
      "Epoch 225/1000\n",
      "108/108 [==============================] - 0s 360us/step - loss: 8.2604e-05 - mse: 8.2604e-05 - val_loss: 4.7462e-04 - val_mse: 4.7462e-04\n",
      "Epoch 226/1000\n",
      "108/108 [==============================] - 0s 402us/step - loss: 5.0666e-05 - mse: 5.0666e-05 - val_loss: 3.9884e-04 - val_mse: 3.9884e-04\n",
      "Epoch 227/1000\n",
      "108/108 [==============================] - 0s 443us/step - loss: 4.3680e-05 - mse: 4.3680e-05 - val_loss: 4.1729e-04 - val_mse: 4.1729e-04\n",
      "Epoch 228/1000\n",
      "108/108 [==============================] - 0s 399us/step - loss: 4.6898e-05 - mse: 4.6898e-05 - val_loss: 4.7808e-04 - val_mse: 4.7808e-04\n",
      "Epoch 229/1000\n",
      "108/108 [==============================] - 0s 436us/step - loss: 4.4637e-05 - mse: 4.4637e-05 - val_loss: 3.0702e-04 - val_mse: 3.0702e-04\n",
      "Epoch 230/1000\n",
      "108/108 [==============================] - 0s 402us/step - loss: 6.3478e-05 - mse: 6.3478e-05 - val_loss: 2.4087e-04 - val_mse: 2.4087e-04\n",
      "Epoch 231/1000\n",
      "108/108 [==============================] - 0s 436us/step - loss: 5.6206e-05 - mse: 5.6206e-05 - val_loss: 2.5927e-04 - val_mse: 2.5927e-04\n",
      "Epoch 232/1000\n",
      "108/108 [==============================] - 0s 439us/step - loss: 6.1195e-05 - mse: 6.1195e-05 - val_loss: 3.6780e-04 - val_mse: 3.6780e-04\n",
      "Epoch 233/1000\n",
      "108/108 [==============================] - 0s 400us/step - loss: 5.4939e-05 - mse: 5.4939e-05 - val_loss: 5.7464e-04 - val_mse: 5.7464e-04\n",
      "Epoch 234/1000\n",
      "108/108 [==============================] - 0s 397us/step - loss: 6.9901e-05 - mse: 6.9901e-05 - val_loss: 3.4864e-04 - val_mse: 3.4864e-04\n",
      "Epoch 235/1000\n",
      "108/108 [==============================] - 0s 392us/step - loss: 7.5044e-05 - mse: 7.5044e-05 - val_loss: 4.3291e-04 - val_mse: 4.3291e-04\n",
      "Epoch 236/1000\n",
      "108/108 [==============================] - 0s 401us/step - loss: 4.4429e-05 - mse: 4.4429e-05 - val_loss: 5.8057e-04 - val_mse: 5.8057e-04\n",
      "Epoch 237/1000\n",
      "108/108 [==============================] - 0s 398us/step - loss: 4.6970e-05 - mse: 4.6970e-05 - val_loss: 4.0600e-04 - val_mse: 4.0600e-04\n",
      "Epoch 238/1000\n",
      "108/108 [==============================] - 0s 392us/step - loss: 5.3965e-05 - mse: 5.3965e-05 - val_loss: 5.4336e-04 - val_mse: 5.4336e-04\n",
      "Epoch 239/1000\n",
      "108/108 [==============================] - 0s 445us/step - loss: 5.0047e-05 - mse: 5.0047e-05 - val_loss: 5.2679e-04 - val_mse: 5.2679e-04\n",
      "Epoch 240/1000\n",
      "108/108 [==============================] - 0s 378us/step - loss: 4.6653e-05 - mse: 4.6653e-05 - val_loss: 3.5699e-04 - val_mse: 3.5699e-04\n",
      "Epoch 241/1000\n",
      "108/108 [==============================] - 0s 448us/step - loss: 5.8862e-05 - mse: 5.8862e-05 - val_loss: 3.3380e-04 - val_mse: 3.3380e-04\n",
      "Epoch 242/1000\n",
      "108/108 [==============================] - 0s 507us/step - loss: 4.5435e-05 - mse: 4.5435e-05 - val_loss: 3.4594e-04 - val_mse: 3.4594e-04\n",
      "Epoch 243/1000\n",
      "108/108 [==============================] - 0s 378us/step - loss: 4.9134e-05 - mse: 4.9134e-05 - val_loss: 4.1810e-04 - val_mse: 4.1810e-04\n",
      "Epoch 244/1000\n",
      "108/108 [==============================] - 0s 406us/step - loss: 4.4768e-05 - mse: 4.4768e-05 - val_loss: 4.7503e-04 - val_mse: 4.7503e-04\n",
      "Epoch 245/1000\n",
      "108/108 [==============================] - 0s 405us/step - loss: 5.8628e-05 - mse: 5.8628e-05 - val_loss: 3.7542e-04 - val_mse: 3.7542e-04\n",
      "Epoch 246/1000\n",
      "108/108 [==============================] - 0s 407us/step - loss: 4.9909e-05 - mse: 4.9909e-05 - val_loss: 3.1316e-04 - val_mse: 3.1316e-04\n",
      "Epoch 247/1000\n",
      "108/108 [==============================] - 0s 525us/step - loss: 6.3006e-05 - mse: 6.3006e-05 - val_loss: 2.1647e-04 - val_mse: 2.1647e-04\n",
      "Epoch 248/1000\n",
      "108/108 [==============================] - 0s 545us/step - loss: 6.1004e-05 - mse: 6.1004e-05 - val_loss: 2.3477e-04 - val_mse: 2.3477e-04\n",
      "Epoch 249/1000\n",
      "108/108 [==============================] - 0s 471us/step - loss: 5.4995e-05 - mse: 5.4995e-05 - val_loss: 3.2707e-04 - val_mse: 3.2707e-04\n",
      "Epoch 250/1000\n",
      "108/108 [==============================] - 0s 423us/step - loss: 5.5603e-05 - mse: 5.5603e-05 - val_loss: 2.4400e-04 - val_mse: 2.4400e-04\n",
      "Epoch 251/1000\n",
      "108/108 [==============================] - 0s 444us/step - loss: 4.4879e-05 - mse: 4.4879e-05 - val_loss: 4.0208e-04 - val_mse: 4.0208e-04\n",
      "Epoch 252/1000\n",
      "108/108 [==============================] - 0s 432us/step - loss: 7.7480e-05 - mse: 7.7480e-05 - val_loss: 3.1508e-04 - val_mse: 3.1508e-04\n",
      "Epoch 253/1000\n",
      "108/108 [==============================] - 0s 381us/step - loss: 5.2313e-05 - mse: 5.2313e-05 - val_loss: 5.2087e-04 - val_mse: 5.2087e-04\n",
      "Epoch 254/1000\n",
      "108/108 [==============================] - 0s 378us/step - loss: 4.6595e-05 - mse: 4.6595e-05 - val_loss: 2.7460e-04 - val_mse: 2.7460e-04\n",
      "Epoch 255/1000\n",
      "108/108 [==============================] - 0s 410us/step - loss: 5.5439e-05 - mse: 5.5439e-05 - val_loss: 2.5245e-04 - val_mse: 2.5245e-04\n",
      "Epoch 256/1000\n",
      "108/108 [==============================] - 0s 402us/step - loss: 4.7018e-05 - mse: 4.7018e-05 - val_loss: 5.3885e-04 - val_mse: 5.3885e-04\n",
      "Epoch 257/1000\n",
      "108/108 [==============================] - 0s 414us/step - loss: 5.8477e-05 - mse: 5.8477e-05 - val_loss: 2.5946e-04 - val_mse: 2.5946e-04\n",
      "Epoch 258/1000\n",
      "108/108 [==============================] - 0s 382us/step - loss: 5.4492e-05 - mse: 5.4492e-05 - val_loss: 3.5519e-04 - val_mse: 3.5519e-04\n",
      "Epoch 259/1000\n",
      "108/108 [==============================] - 0s 416us/step - loss: 4.8080e-05 - mse: 4.8080e-05 - val_loss: 5.7940e-04 - val_mse: 5.7940e-04\n",
      "Epoch 260/1000\n",
      "108/108 [==============================] - 0s 419us/step - loss: 5.3414e-05 - mse: 5.3414e-05 - val_loss: 5.2535e-04 - val_mse: 5.2535e-04\n",
      "Epoch 261/1000\n",
      "108/108 [==============================] - 0s 425us/step - loss: 5.3911e-05 - mse: 5.3911e-05 - val_loss: 3.0560e-04 - val_mse: 3.0560e-04\n",
      "Epoch 262/1000\n",
      "108/108 [==============================] - 0s 410us/step - loss: 4.5882e-05 - mse: 4.5882e-05 - val_loss: 6.1458e-04 - val_mse: 6.1458e-04\n",
      "Epoch 263/1000\n",
      "108/108 [==============================] - 0s 417us/step - loss: 8.3524e-05 - mse: 8.3524e-05 - val_loss: 4.8698e-04 - val_mse: 4.8698e-04\n",
      "Epoch 264/1000\n",
      "108/108 [==============================] - 0s 392us/step - loss: 3.9738e-05 - mse: 3.9738e-05 - val_loss: 4.3667e-04 - val_mse: 4.3667e-04\n",
      "Epoch 265/1000\n",
      "108/108 [==============================] - 0s 455us/step - loss: 5.6588e-05 - mse: 5.6588e-05 - val_loss: 3.8870e-04 - val_mse: 3.8870e-04\n",
      "Epoch 266/1000\n",
      "108/108 [==============================] - 0s 363us/step - loss: 4.5479e-05 - mse: 4.5479e-05 - val_loss: 2.3261e-04 - val_mse: 2.3261e-04\n",
      "Epoch 267/1000\n",
      "108/108 [==============================] - 0s 363us/step - loss: 5.6951e-05 - mse: 5.6951e-05 - val_loss: 4.1653e-04 - val_mse: 4.1653e-04\n",
      "Epoch 268/1000\n",
      "108/108 [==============================] - 0s 463us/step - loss: 4.1986e-05 - mse: 4.1986e-05 - val_loss: 3.8037e-04 - val_mse: 3.8037e-04\n",
      "Epoch 269/1000\n",
      "108/108 [==============================] - 0s 371us/step - loss: 5.7195e-05 - mse: 5.7195e-05 - val_loss: 2.4409e-04 - val_mse: 2.4409e-04\n",
      "Epoch 270/1000\n",
      "108/108 [==============================] - 0s 444us/step - loss: 4.6168e-05 - mse: 4.6168e-05 - val_loss: 2.8415e-04 - val_mse: 2.8415e-04\n",
      "Epoch 271/1000\n",
      "108/108 [==============================] - 0s 369us/step - loss: 5.0971e-05 - mse: 5.0971e-05 - val_loss: 3.7832e-04 - val_mse: 3.7832e-04\n",
      "Epoch 272/1000\n",
      "108/108 [==============================] - 0s 422us/step - loss: 4.4685e-05 - mse: 4.4685e-05 - val_loss: 2.5357e-04 - val_mse: 2.5357e-04\n",
      "Epoch 273/1000\n",
      "108/108 [==============================] - 0s 434us/step - loss: 4.5887e-05 - mse: 4.5887e-05 - val_loss: 4.1900e-04 - val_mse: 4.1900e-04\n",
      "Epoch 274/1000\n",
      "108/108 [==============================] - 0s 452us/step - loss: 4.6911e-05 - mse: 4.6911e-05 - val_loss: 4.6699e-04 - val_mse: 4.6699e-04\n",
      "Epoch 275/1000\n",
      "108/108 [==============================] - 0s 430us/step - loss: 5.7919e-05 - mse: 5.7919e-05 - val_loss: 3.9663e-04 - val_mse: 3.9663e-04\n",
      "Epoch 276/1000\n",
      "108/108 [==============================] - 0s 477us/step - loss: 5.8053e-05 - mse: 5.8053e-05 - val_loss: 1.9175e-04 - val_mse: 1.9175e-04\n",
      "Epoch 277/1000\n",
      "108/108 [==============================] - 0s 398us/step - loss: 5.4991e-05 - mse: 5.4991e-05 - val_loss: 3.9540e-04 - val_mse: 3.9540e-04\n",
      "Epoch 278/1000\n",
      "108/108 [==============================] - 0s 497us/step - loss: 4.7545e-05 - mse: 4.7545e-05 - val_loss: 3.7015e-04 - val_mse: 3.7015e-04\n",
      "Epoch 279/1000\n",
      "108/108 [==============================] - 0s 438us/step - loss: 4.0814e-05 - mse: 4.0814e-05 - val_loss: 3.5857e-04 - val_mse: 3.5857e-04\n",
      "Epoch 280/1000\n",
      "108/108 [==============================] - 0s 875us/step - loss: 4.0053e-05 - mse: 4.0053e-05 - val_loss: 3.5005e-04 - val_mse: 3.5005e-04\n",
      "Epoch 281/1000\n",
      "108/108 [==============================] - 0s 492us/step - loss: 4.7159e-05 - mse: 4.7159e-05 - val_loss: 4.9917e-04 - val_mse: 4.9917e-04\n",
      "Epoch 282/1000\n",
      "108/108 [==============================] - 0s 426us/step - loss: 6.3358e-05 - mse: 6.3358e-05 - val_loss: 2.8241e-04 - val_mse: 2.8241e-04\n",
      "Epoch 283/1000\n",
      "108/108 [==============================] - 0s 390us/step - loss: 5.4115e-05 - mse: 5.4115e-05 - val_loss: 2.0439e-04 - val_mse: 2.0439e-04\n",
      "Epoch 284/1000\n",
      "108/108 [==============================] - 0s 390us/step - loss: 3.7868e-05 - mse: 3.7868e-05 - val_loss: 3.4188e-04 - val_mse: 3.4188e-04\n",
      "Epoch 285/1000\n",
      "108/108 [==============================] - 0s 484us/step - loss: 4.9141e-05 - mse: 4.9141e-05 - val_loss: 4.8343e-04 - val_mse: 4.8343e-04\n",
      "Epoch 286/1000\n",
      "108/108 [==============================] - 0s 392us/step - loss: 5.3445e-05 - mse: 5.3445e-05 - val_loss: 4.6604e-04 - val_mse: 4.6604e-04\n",
      "Epoch 287/1000\n",
      "108/108 [==============================] - 0s 408us/step - loss: 3.9850e-05 - mse: 3.9850e-05 - val_loss: 2.0554e-04 - val_mse: 2.0554e-04\n",
      "Epoch 288/1000\n",
      "108/108 [==============================] - 0s 420us/step - loss: 6.1638e-05 - mse: 6.1638e-05 - val_loss: 3.7225e-04 - val_mse: 3.7225e-04\n",
      "Epoch 289/1000\n",
      "108/108 [==============================] - 0s 462us/step - loss: 4.8674e-05 - mse: 4.8674e-05 - val_loss: 3.3196e-04 - val_mse: 3.3196e-04\n",
      "Epoch 290/1000\n",
      "108/108 [==============================] - 0s 388us/step - loss: 4.4951e-05 - mse: 4.4951e-05 - val_loss: 5.8466e-04 - val_mse: 5.8466e-04\n",
      "Epoch 291/1000\n",
      "108/108 [==============================] - 0s 373us/step - loss: 5.0858e-05 - mse: 5.0858e-05 - val_loss: 3.1049e-04 - val_mse: 3.1049e-04\n",
      "Epoch 292/1000\n",
      "108/108 [==============================] - 0s 523us/step - loss: 4.9963e-05 - mse: 4.9963e-05 - val_loss: 1.4435e-04 - val_mse: 1.4435e-04\n",
      "Epoch 293/1000\n",
      "108/108 [==============================] - 0s 369us/step - loss: 6.3590e-05 - mse: 6.3590e-05 - val_loss: 4.4920e-04 - val_mse: 4.4920e-04\n",
      "Epoch 294/1000\n",
      "108/108 [==============================] - 0s 435us/step - loss: 4.9084e-05 - mse: 4.9084e-05 - val_loss: 2.9493e-04 - val_mse: 2.9493e-04\n",
      "Epoch 295/1000\n",
      "108/108 [==============================] - 0s 360us/step - loss: 5.5467e-05 - mse: 5.5467e-05 - val_loss: 1.7387e-04 - val_mse: 1.7387e-04\n",
      "Epoch 296/1000\n",
      "108/108 [==============================] - 0s 447us/step - loss: 6.1870e-05 - mse: 6.1870e-05 - val_loss: 2.1034e-04 - val_mse: 2.1034e-04\n",
      "Epoch 297/1000\n",
      "108/108 [==============================] - 0s 409us/step - loss: 4.8210e-05 - mse: 4.8210e-05 - val_loss: 2.4664e-04 - val_mse: 2.4664e-04\n",
      "Epoch 298/1000\n",
      "108/108 [==============================] - 0s 422us/step - loss: 3.6625e-05 - mse: 3.6625e-05 - val_loss: 2.1434e-04 - val_mse: 2.1434e-04\n",
      "Epoch 299/1000\n",
      "108/108 [==============================] - 0s 386us/step - loss: 5.6186e-05 - mse: 5.6186e-05 - val_loss: 3.7063e-04 - val_mse: 3.7063e-04\n",
      "Epoch 300/1000\n",
      "108/108 [==============================] - 0s 427us/step - loss: 6.3204e-05 - mse: 6.3204e-05 - val_loss: 2.6359e-04 - val_mse: 2.6359e-04\n",
      "Epoch 301/1000\n",
      "108/108 [==============================] - 0s 434us/step - loss: 3.9567e-05 - mse: 3.9567e-05 - val_loss: 1.4775e-04 - val_mse: 1.4775e-04\n",
      "Epoch 302/1000\n",
      "108/108 [==============================] - 0s 365us/step - loss: 4.7440e-05 - mse: 4.7440e-05 - val_loss: 2.4237e-04 - val_mse: 2.4237e-04\n",
      "Epoch 303/1000\n",
      "108/108 [==============================] - 0s 425us/step - loss: 4.5851e-05 - mse: 4.5851e-05 - val_loss: 2.6417e-04 - val_mse: 2.6417e-04\n",
      "Epoch 304/1000\n",
      "108/108 [==============================] - 0s 437us/step - loss: 5.0889e-05 - mse: 5.0889e-05 - val_loss: 5.4580e-04 - val_mse: 5.4580e-04\n",
      "Epoch 305/1000\n",
      "108/108 [==============================] - 0s 429us/step - loss: 5.0759e-05 - mse: 5.0759e-05 - val_loss: 2.2590e-04 - val_mse: 2.2590e-04\n",
      "Epoch 306/1000\n",
      "108/108 [==============================] - 0s 436us/step - loss: 3.7364e-05 - mse: 3.7364e-05 - val_loss: 2.9602e-04 - val_mse: 2.9602e-04\n",
      "Epoch 307/1000\n",
      "108/108 [==============================] - 0s 383us/step - loss: 4.4734e-05 - mse: 4.4734e-05 - val_loss: 4.4540e-04 - val_mse: 4.4540e-04\n",
      "Epoch 308/1000\n",
      "108/108 [==============================] - 0s 427us/step - loss: 4.8736e-05 - mse: 4.8736e-05 - val_loss: 1.7996e-04 - val_mse: 1.7996e-04\n",
      "Epoch 309/1000\n",
      "108/108 [==============================] - 0s 371us/step - loss: 4.3373e-05 - mse: 4.3373e-05 - val_loss: 2.7288e-04 - val_mse: 2.7288e-04\n",
      "Epoch 310/1000\n",
      "108/108 [==============================] - 0s 441us/step - loss: 4.6289e-05 - mse: 4.6289e-05 - val_loss: 2.5347e-04 - val_mse: 2.5347e-04\n",
      "Epoch 311/1000\n",
      "108/108 [==============================] - 0s 408us/step - loss: 4.0729e-05 - mse: 4.0729e-05 - val_loss: 3.2282e-04 - val_mse: 3.2282e-04\n",
      "Epoch 312/1000\n",
      "108/108 [==============================] - 0s 390us/step - loss: 5.2629e-05 - mse: 5.2629e-05 - val_loss: 3.6446e-04 - val_mse: 3.6446e-04\n",
      "Epoch 313/1000\n",
      "108/108 [==============================] - 0s 456us/step - loss: 4.0623e-05 - mse: 4.0623e-05 - val_loss: 2.2299e-04 - val_mse: 2.2299e-04\n",
      "Epoch 314/1000\n",
      "108/108 [==============================] - 0s 332us/step - loss: 5.4087e-05 - mse: 5.4087e-05 - val_loss: 4.9137e-04 - val_mse: 4.9137e-04\n",
      "Epoch 315/1000\n",
      "108/108 [==============================] - 0s 404us/step - loss: 5.2195e-05 - mse: 5.2195e-05 - val_loss: 2.8680e-04 - val_mse: 2.8680e-04\n",
      "Epoch 316/1000\n",
      "108/108 [==============================] - 0s 443us/step - loss: 4.5628e-05 - mse: 4.5628e-05 - val_loss: 3.5644e-04 - val_mse: 3.5644e-04\n",
      "Epoch 317/1000\n",
      "108/108 [==============================] - 0s 381us/step - loss: 5.0489e-05 - mse: 5.0489e-05 - val_loss: 2.3303e-04 - val_mse: 2.3303e-04\n",
      "Epoch 318/1000\n",
      "108/108 [==============================] - 0s 455us/step - loss: 4.1108e-05 - mse: 4.1108e-05 - val_loss: 3.0267e-04 - val_mse: 3.0267e-04\n",
      "Epoch 319/1000\n",
      "108/108 [==============================] - 0s 437us/step - loss: 3.5152e-05 - mse: 3.5152e-05 - val_loss: 2.6084e-04 - val_mse: 2.6084e-04\n",
      "Epoch 320/1000\n",
      "108/108 [==============================] - 0s 435us/step - loss: 3.4131e-05 - mse: 3.4131e-05 - val_loss: 4.3518e-04 - val_mse: 4.3518e-04\n",
      "Epoch 321/1000\n",
      "108/108 [==============================] - 0s 391us/step - loss: 6.3376e-05 - mse: 6.3376e-05 - val_loss: 4.9561e-04 - val_mse: 4.9561e-04\n",
      "Epoch 322/1000\n",
      "108/108 [==============================] - 0s 470us/step - loss: 4.4853e-05 - mse: 4.4853e-05 - val_loss: 2.2849e-04 - val_mse: 2.2849e-04\n",
      "Epoch 323/1000\n",
      "108/108 [==============================] - 0s 361us/step - loss: 3.9101e-05 - mse: 3.9101e-05 - val_loss: 2.3216e-04 - val_mse: 2.3216e-04\n",
      "Epoch 324/1000\n",
      "108/108 [==============================] - 0s 446us/step - loss: 5.6021e-05 - mse: 5.6021e-05 - val_loss: 3.3379e-04 - val_mse: 3.3379e-04\n",
      "Epoch 325/1000\n",
      "108/108 [==============================] - 0s 419us/step - loss: 3.7093e-05 - mse: 3.7093e-05 - val_loss: 2.0595e-04 - val_mse: 2.0595e-04\n",
      "Epoch 326/1000\n",
      "108/108 [==============================] - 0s 408us/step - loss: 3.0086e-05 - mse: 3.0086e-05 - val_loss: 2.9437e-04 - val_mse: 2.9437e-04\n",
      "Epoch 327/1000\n",
      "108/108 [==============================] - 0s 452us/step - loss: 3.9337e-05 - mse: 3.9337e-05 - val_loss: 2.9309e-04 - val_mse: 2.9309e-04\n",
      "Epoch 328/1000\n",
      "108/108 [==============================] - 0s 372us/step - loss: 5.0563e-05 - mse: 5.0563e-05 - val_loss: 3.3265e-04 - val_mse: 3.3265e-04\n",
      "Epoch 329/1000\n",
      "108/108 [==============================] - 0s 430us/step - loss: 4.2755e-05 - mse: 4.2755e-05 - val_loss: 2.1785e-04 - val_mse: 2.1785e-04\n",
      "Epoch 330/1000\n",
      "108/108 [==============================] - 0s 371us/step - loss: 4.6526e-05 - mse: 4.6526e-05 - val_loss: 4.0809e-04 - val_mse: 4.0809e-04\n",
      "Epoch 331/1000\n",
      "108/108 [==============================] - 0s 419us/step - loss: 6.2505e-05 - mse: 6.2505e-05 - val_loss: 4.4085e-04 - val_mse: 4.4085e-04\n",
      "Epoch 332/1000\n",
      "108/108 [==============================] - 0s 373us/step - loss: 3.2782e-05 - mse: 3.2782e-05 - val_loss: 2.8256e-04 - val_mse: 2.8256e-04\n",
      "Epoch 333/1000\n",
      "108/108 [==============================] - 0s 418us/step - loss: 3.8877e-05 - mse: 3.8877e-05 - val_loss: 3.1292e-04 - val_mse: 3.1292e-04\n",
      "Epoch 334/1000\n",
      "108/108 [==============================] - 0s 399us/step - loss: 2.8697e-05 - mse: 2.8697e-05 - val_loss: 2.8499e-04 - val_mse: 2.8499e-04\n",
      "Epoch 335/1000\n",
      "108/108 [==============================] - 0s 410us/step - loss: 3.7645e-05 - mse: 3.7645e-05 - val_loss: 1.8348e-04 - val_mse: 1.8348e-04\n",
      "Epoch 336/1000\n",
      "108/108 [==============================] - 0s 424us/step - loss: 6.1161e-05 - mse: 6.1161e-05 - val_loss: 3.1298e-04 - val_mse: 3.1298e-04\n",
      "Epoch 337/1000\n",
      "108/108 [==============================] - 0s 391us/step - loss: 3.8307e-05 - mse: 3.8307e-05 - val_loss: 1.6139e-04 - val_mse: 1.6139e-04\n",
      "Epoch 338/1000\n",
      "108/108 [==============================] - 0s 448us/step - loss: 4.9864e-05 - mse: 4.9864e-05 - val_loss: 4.8296e-04 - val_mse: 4.8296e-04\n",
      "Epoch 339/1000\n",
      "108/108 [==============================] - 0s 388us/step - loss: 5.5679e-05 - mse: 5.5679e-05 - val_loss: 3.1457e-04 - val_mse: 3.1457e-04\n",
      "Epoch 340/1000\n",
      "108/108 [==============================] - 0s 373us/step - loss: 3.4322e-05 - mse: 3.4322e-05 - val_loss: 2.9446e-04 - val_mse: 2.9446e-04\n",
      "Epoch 341/1000\n",
      "108/108 [==============================] - 0s 489us/step - loss: 3.5128e-05 - mse: 3.5128e-05 - val_loss: 2.4386e-04 - val_mse: 2.4386e-04\n",
      "Epoch 342/1000\n",
      "108/108 [==============================] - 0s 361us/step - loss: 4.4288e-05 - mse: 4.4288e-05 - val_loss: 2.1763e-04 - val_mse: 2.1763e-04\n",
      "Epoch 343/1000\n",
      "108/108 [==============================] - 0s 391us/step - loss: 4.7570e-05 - mse: 4.7570e-05 - val_loss: 2.0806e-04 - val_mse: 2.0806e-04\n",
      "Epoch 344/1000\n",
      "108/108 [==============================] - 0s 427us/step - loss: 3.3733e-05 - mse: 3.3733e-05 - val_loss: 1.6645e-04 - val_mse: 1.6645e-04\n",
      "Epoch 345/1000\n",
      "108/108 [==============================] - 0s 409us/step - loss: 3.8994e-05 - mse: 3.8994e-05 - val_loss: 2.4884e-04 - val_mse: 2.4884e-04\n",
      "Epoch 346/1000\n",
      "108/108 [==============================] - 0s 435us/step - loss: 3.7777e-05 - mse: 3.7777e-05 - val_loss: 2.5645e-04 - val_mse: 2.5645e-04\n",
      "Epoch 347/1000\n",
      "108/108 [==============================] - 0s 376us/step - loss: 3.6920e-05 - mse: 3.6920e-05 - val_loss: 4.8324e-04 - val_mse: 4.8324e-04\n",
      "Epoch 348/1000\n",
      "108/108 [==============================] - 0s 428us/step - loss: 3.4840e-05 - mse: 3.4840e-05 - val_loss: 2.0204e-04 - val_mse: 2.0204e-04\n",
      "Epoch 349/1000\n",
      "108/108 [==============================] - 0s 447us/step - loss: 3.1382e-05 - mse: 3.1382e-05 - val_loss: 1.8904e-04 - val_mse: 1.8904e-04\n",
      "Epoch 350/1000\n",
      "108/108 [==============================] - 0s 389us/step - loss: 4.6893e-05 - mse: 4.6893e-05 - val_loss: 2.7849e-04 - val_mse: 2.7849e-04\n",
      "Epoch 351/1000\n",
      "108/108 [==============================] - 0s 419us/step - loss: 4.4424e-05 - mse: 4.4424e-05 - val_loss: 2.1156e-04 - val_mse: 2.1156e-04\n",
      "Epoch 352/1000\n",
      "108/108 [==============================] - 0s 512us/step - loss: 3.9073e-05 - mse: 3.9073e-05 - val_loss: 2.6273e-04 - val_mse: 2.6273e-04\n",
      "Epoch 353/1000\n",
      "108/108 [==============================] - 0s 476us/step - loss: 3.2373e-05 - mse: 3.2373e-05 - val_loss: 5.3170e-04 - val_mse: 5.3170e-04\n",
      "Epoch 354/1000\n",
      "108/108 [==============================] - 0s 487us/step - loss: 5.6455e-05 - mse: 5.6455e-05 - val_loss: 4.1329e-04 - val_mse: 4.1329e-04\n",
      "Epoch 355/1000\n",
      "108/108 [==============================] - 0s 503us/step - loss: 4.0009e-05 - mse: 4.0009e-05 - val_loss: 3.5289e-04 - val_mse: 3.5289e-04\n",
      "Epoch 356/1000\n",
      "108/108 [==============================] - 0s 549us/step - loss: 3.9293e-05 - mse: 3.9293e-05 - val_loss: 3.7662e-04 - val_mse: 3.7662e-04\n",
      "Epoch 357/1000\n",
      "108/108 [==============================] - 0s 385us/step - loss: 3.9907e-05 - mse: 3.9907e-05 - val_loss: 3.9121e-04 - val_mse: 3.9121e-04\n",
      "Epoch 358/1000\n",
      "108/108 [==============================] - 0s 425us/step - loss: 3.4411e-05 - mse: 3.4411e-05 - val_loss: 2.5901e-04 - val_mse: 2.5901e-04\n",
      "Epoch 359/1000\n",
      "108/108 [==============================] - 0s 413us/step - loss: 4.6494e-05 - mse: 4.6494e-05 - val_loss: 2.4295e-04 - val_mse: 2.4295e-04\n",
      "Epoch 360/1000\n",
      "108/108 [==============================] - 0s 464us/step - loss: 4.8299e-05 - mse: 4.8299e-05 - val_loss: 3.1673e-04 - val_mse: 3.1673e-04\n",
      "Epoch 361/1000\n",
      "108/108 [==============================] - 0s 378us/step - loss: 4.3285e-05 - mse: 4.3285e-05 - val_loss: 3.6286e-04 - val_mse: 3.6286e-04\n",
      "Epoch 362/1000\n",
      "108/108 [==============================] - 0s 444us/step - loss: 4.5223e-05 - mse: 4.5223e-05 - val_loss: 2.9410e-04 - val_mse: 2.9410e-04\n",
      "Epoch 363/1000\n",
      "108/108 [==============================] - 0s 352us/step - loss: 3.7035e-05 - mse: 3.7035e-05 - val_loss: 3.7390e-04 - val_mse: 3.7390e-04\n",
      "Epoch 364/1000\n",
      "108/108 [==============================] - 0s 439us/step - loss: 3.7564e-05 - mse: 3.7564e-05 - val_loss: 2.3016e-04 - val_mse: 2.3016e-04\n",
      "Epoch 365/1000\n",
      "108/108 [==============================] - 0s 404us/step - loss: 3.4491e-05 - mse: 3.4491e-05 - val_loss: 2.2131e-04 - val_mse: 2.2131e-04\n",
      "Epoch 366/1000\n",
      "108/108 [==============================] - 0s 413us/step - loss: 3.6286e-05 - mse: 3.6286e-05 - val_loss: 3.3898e-04 - val_mse: 3.3898e-04\n",
      "Epoch 367/1000\n",
      "108/108 [==============================] - 0s 397us/step - loss: 3.5767e-05 - mse: 3.5767e-05 - val_loss: 2.6520e-04 - val_mse: 2.6520e-04\n",
      "Epoch 368/1000\n",
      "108/108 [==============================] - 0s 428us/step - loss: 3.2908e-05 - mse: 3.2908e-05 - val_loss: 2.2942e-04 - val_mse: 2.2942e-04\n",
      "Epoch 369/1000\n",
      "108/108 [==============================] - 0s 534us/step - loss: 3.7993e-05 - mse: 3.7993e-05 - val_loss: 1.4332e-04 - val_mse: 1.4332e-04\n",
      "Epoch 370/1000\n",
      "108/108 [==============================] - 0s 378us/step - loss: 3.5550e-05 - mse: 3.5550e-05 - val_loss: 3.8076e-04 - val_mse: 3.8076e-04\n",
      "Epoch 371/1000\n",
      "108/108 [==============================] - 0s 440us/step - loss: 3.3848e-05 - mse: 3.3848e-05 - val_loss: 1.8479e-04 - val_mse: 1.8479e-04\n",
      "Epoch 372/1000\n",
      "108/108 [==============================] - 0s 388us/step - loss: 3.7028e-05 - mse: 3.7028e-05 - val_loss: 1.6577e-04 - val_mse: 1.6577e-04\n",
      "Epoch 373/1000\n",
      "108/108 [==============================] - 0s 443us/step - loss: 4.5615e-05 - mse: 4.5615e-05 - val_loss: 1.9403e-04 - val_mse: 1.9403e-04\n",
      "Epoch 374/1000\n",
      "108/108 [==============================] - 0s 434us/step - loss: 3.0951e-05 - mse: 3.0951e-05 - val_loss: 2.1500e-04 - val_mse: 2.1500e-04\n",
      "Epoch 375/1000\n",
      "108/108 [==============================] - 0s 465us/step - loss: 5.8084e-05 - mse: 5.8084e-05 - val_loss: 3.8652e-04 - val_mse: 3.8652e-04\n",
      "Epoch 376/1000\n",
      "108/108 [==============================] - 0s 400us/step - loss: 4.6624e-05 - mse: 4.6624e-05 - val_loss: 2.8856e-04 - val_mse: 2.8856e-04\n",
      "Epoch 377/1000\n",
      "108/108 [==============================] - 0s 386us/step - loss: 4.6527e-05 - mse: 4.6527e-05 - val_loss: 1.9854e-04 - val_mse: 1.9854e-04\n",
      "Epoch 378/1000\n",
      "108/108 [==============================] - 0s 407us/step - loss: 2.9515e-05 - mse: 2.9515e-05 - val_loss: 2.5063e-04 - val_mse: 2.5063e-04\n",
      "Epoch 379/1000\n",
      "108/108 [==============================] - 0s 445us/step - loss: 3.1134e-05 - mse: 3.1134e-05 - val_loss: 1.8808e-04 - val_mse: 1.8808e-04\n",
      "Epoch 380/1000\n",
      "108/108 [==============================] - 0s 380us/step - loss: 3.0834e-05 - mse: 3.0834e-05 - val_loss: 2.0829e-04 - val_mse: 2.0829e-04\n",
      "Epoch 381/1000\n",
      "108/108 [==============================] - 0s 351us/step - loss: 2.7353e-05 - mse: 2.7353e-05 - val_loss: 2.6786e-04 - val_mse: 2.6786e-04\n",
      "Epoch 382/1000\n",
      "108/108 [==============================] - 0s 401us/step - loss: 3.5470e-05 - mse: 3.5470e-05 - val_loss: 2.6073e-04 - val_mse: 2.6073e-04\n",
      "Epoch 383/1000\n",
      "108/108 [==============================] - 0s 477us/step - loss: 4.8878e-05 - mse: 4.8878e-05 - val_loss: 2.3099e-04 - val_mse: 2.3099e-04\n",
      "Epoch 384/1000\n",
      "108/108 [==============================] - 0s 500us/step - loss: 5.1042e-05 - mse: 5.1042e-05 - val_loss: 2.6605e-04 - val_mse: 2.6605e-04\n",
      "Epoch 385/1000\n",
      "108/108 [==============================] - 0s 476us/step - loss: 2.1149e-05 - mse: 2.1149e-05 - val_loss: 2.2003e-04 - val_mse: 2.2003e-04\n",
      "Epoch 386/1000\n",
      "108/108 [==============================] - 0s 408us/step - loss: 3.9524e-05 - mse: 3.9524e-05 - val_loss: 2.8181e-04 - val_mse: 2.8181e-04\n",
      "Epoch 387/1000\n",
      "108/108 [==============================] - 0s 369us/step - loss: 3.3933e-05 - mse: 3.3933e-05 - val_loss: 2.7093e-04 - val_mse: 2.7093e-04\n",
      "Epoch 388/1000\n",
      "108/108 [==============================] - 0s 419us/step - loss: 4.4360e-05 - mse: 4.4360e-05 - val_loss: 3.1407e-04 - val_mse: 3.1407e-04\n",
      "Epoch 389/1000\n",
      "108/108 [==============================] - 0s 479us/step - loss: 4.3505e-05 - mse: 4.3505e-05 - val_loss: 2.4138e-04 - val_mse: 2.4138e-04\n",
      "Epoch 390/1000\n",
      "108/108 [==============================] - 0s 415us/step - loss: 5.4408e-05 - mse: 5.4408e-05 - val_loss: 3.8624e-04 - val_mse: 3.8624e-04\n",
      "Epoch 391/1000\n",
      "108/108 [==============================] - 0s 360us/step - loss: 3.8752e-05 - mse: 3.8752e-05 - val_loss: 2.8451e-04 - val_mse: 2.8451e-04\n",
      "Epoch 392/1000\n",
      "108/108 [==============================] - 0s 410us/step - loss: 2.6983e-05 - mse: 2.6983e-05 - val_loss: 2.2156e-04 - val_mse: 2.2156e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, np.array(y_train), batch_size=16, epochs=1000, callbacks=callbacks, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./checkpoints/mlp.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017956887113815267\n",
      "1.6895058562206642\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from utils.eval import get_ia, get_mae, get_mse, get_rmse, get_r, get_u95\n",
    "\n",
    "# y_pred = linear_reg.predict(poly_reg.fit_transform(x_test))\n",
    "\n",
    "pred_result = model.predict(x_valid)\n",
    "pred_result = scaler_y.inverse_transform(pred_result)\n",
    "\n",
    "y_pred = pred_result[:, 0]\n",
    "y_true = y_valid_copy[:, 0]\n",
    "y_pred[y_pred > 1.0] = 1.0\n",
    "y_pred[y_pred < 0.0] = 0.0\n",
    "\n",
    "# print(y_pred, y_true)\n",
    "print(np.sqrt(mean_squared_error(y_pred, y_true)))\n",
    "\n",
    "y_pred = pred_result[:, 1]\n",
    "y_true = y_valid_copy[:, 1]\n",
    "\n",
    "for i in range(x_valid.shape[0]):\n",
    "    if y_pred[i] > x_valid_copy[i, 1]:\n",
    "        y_pred[i] = x_valid_copy[i, 1]\n",
    "\n",
    "# print(y_pred, y_true)\n",
    "print(np.sqrt(mean_squared_error(y_pred, y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction mae: 0.011700655060106162\n",
      "fraction mse: 0.00021548268690297772\n",
      "fraction rmse: 0.014679328557634293\n",
      "---------------------------------------------------\n",
      "temperature mae: 0.33723566470516436\n",
      "temperature mse: 0.16480848815349694\n",
      "temperature rmse: 0.4059661170017726\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.eval import get_ia, get_mae, get_mse, get_rmse, get_r, get_u95, get_mre\n",
    "\n",
    "def model_predict_from_csv(data_eval):\n",
    "\n",
    "    # x_test = np.array(pd.read_csv(data_eval))[:, :3]\n",
    "    # y_test = np.array(pd.read_csv(data_eval))[:, 3:]\n",
    "\n",
    "    # x_test_copy = x_test.copy()\n",
    "    # x_test = scaler.transform(x_test)\n",
    "    \n",
    "    # print(data_copy)\n",
    "\n",
    "    frac_true = y_test_copy[:, 0]\n",
    "    temp_pcm_true = y_test_copy[:, 1]\n",
    "\n",
    "    pred_result = model.predict(x_test)\n",
    "    pred_result = scaler_y.inverse_transform(pred_result)\n",
    "    \n",
    "    frac_pred = pred_result[:, 0]\n",
    "    frac_pred[frac_pred > 1.0] = 1.0  # 11\n",
    "    frac_pred[frac_pred < 0.0] = 0.0\n",
    "    temp_pcm_pred = pred_result[:, 1]\n",
    "    for i in range(x_test.shape[0]):\n",
    "        temp_pcm_pred[i] = np.min((temp_pcm_pred[i], x_test_copy[i, 1]))\n",
    "\n",
    "    print (\"fraction mae:\", mean_absolute_error(frac_true, frac_pred))\n",
    "    print (\"fraction mse:\", mean_squared_error(frac_true, frac_pred))\n",
    "    print (\"fraction rmse:\", np.sqrt(mean_squared_error(frac_true, frac_pred)))\n",
    "    print('---------------------------------------------------')\n",
    "    print (\"temperature mae:\", mean_absolute_error(temp_pcm_true, temp_pcm_pred))\n",
    "    print (\"temperature mse:\", mean_squared_error(temp_pcm_true, temp_pcm_pred))\n",
    "    print (\"temperature rmse:\", np.sqrt(mean_squared_error(temp_pcm_true, temp_pcm_pred)))\n",
    "\n",
    "    result_df = pd.DataFrame({\"velo_in\": x_test_copy[:, 0], \"temp_in\": x_test_copy[:, 1], \"time\": x_test_copy[:, 2], \"frac_true\": frac_true, \"frac_pred\": frac_pred, \"temp_pcm_true\": temp_pcm_true, \"temp_pcm_pred\":temp_pcm_pred})\n",
    "    result_df.to_csv(f\"./results/predict_from_csv_result_mlp.csv\", index=False)\n",
    "\n",
    "    return None\n",
    "\n",
    "data_eval = './data_eval_v_0.7.csv'\n",
    "model_predict_from_csv(data_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.eval import get_ia, get_mae, get_mse, get_rmse, get_r, get_u95, get_mre\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(result_file):\n",
    "    result_df = pd.read_csv(result_file)\n",
    "    eval_df = pd.DataFrame(columns=['velo_in', 'temp_in', 'len_data', 'r_frac', 'mae_frac', 'rmse_frac', 'ia_frac', 'u95_frac', 'mre_frac', 'r_temp', 'mae_temp', 'rmse_temp', 'ia_temp', 'u95_temp', 'mre_temp'])\n",
    "    \n",
    "    # velo_in_config = [0.7, 0.8, 0.9, 1.0]\n",
    "    velo_in_config = [0.7,]\n",
    "    temp_in_config = [80.0, 85.0, 90.0, 95.0, 100.0]\n",
    "\n",
    "    for v in velo_in_config:\n",
    "        for t in temp_in_config:\n",
    "            frac_true = []\n",
    "            frac_pred = []\n",
    "            temp_true = []\n",
    "            temp_pred = []\n",
    "            time_list = []\n",
    "            for i in range(len(result_df)):\n",
    "                if result_df['velo_in'][i] == v and result_df['temp_in'][i] == t:\n",
    "                    frac_true.append(result_df['frac_true'][i])\n",
    "                    frac_pred.append(result_df['frac_pred'][i])\n",
    "                    temp_true.append(result_df['temp_pcm_true'][i]) \n",
    "                    temp_pred.append(result_df['temp_pcm_pred'][i])\n",
    "                    time_list.append(result_df['time'][i])\n",
    "            \n",
    "            plt.clf()\n",
    "            if not len(frac_true) == 0:\n",
    "\n",
    "                plt.clf()\n",
    "                plt.figure(figsize=(9, 6), dpi=100)\n",
    "                plt.title(f'Velo {v} Temp {t} fraction')\n",
    "                plt.xlabel('time(s)')\n",
    "                plt.ylabel('Fraction of liquid phase')\n",
    "                plt.xlim(0, 1000)\n",
    "                plt.ylim(0, 1.1)\n",
    "                # print(time_list)\n",
    "                plt.plot(time_list, frac_true, label='frac true')\n",
    "                plt.plot(time_list, frac_pred, label='frac pred')\n",
    "                plt.legend(loc='lower right')\n",
    "                plt.savefig(f'./figures/Frac_{v}_{t}.jpg')\n",
    "\n",
    "                plt.clf()\n",
    "                plt.figure(figsize=(9, 6), dpi=100)\n",
    "                plt.title(f'Velo {v} Temp {t} Temperature')\n",
    "                plt.xlabel('time (s)')\n",
    "                plt.ylabel('Average temperature of PCM ()')\n",
    "                plt.xlim(0, 1000)\n",
    "                plt.ylim(20, 100)\n",
    "                plt.plot(time_list, temp_true, label='temp true')\n",
    "                plt.plot(time_list, temp_pred, label='temp pred')\n",
    "                plt.legend(loc='lower right')\n",
    "                plt.savefig(f'./figures/Temp_{v}_{t}.jpg')\n",
    "            \n",
    "                # r_frac = get_r(frac_true, frac_pred)\n",
    "                # mae_frac = get_mae(frac_true, frac_pred)\n",
    "                # rmse_frac = get_rmse(frac_true, frac_pred)\n",
    "                # ia_frac = get_ia(frac_true, frac_pred)\n",
    "                # u95_frac = get_u95(frac_true, frac_pred)\n",
    "                # mre_frac = get_mre(frac_true, frac_pred)\n",
    "                # r_temp = get_r(temp_true, temp_pred)\n",
    "                # mae_temp = get_mae(temp_true, temp_pred)\n",
    "                # rmse_temp = get_rmse(temp_true, temp_pred)\n",
    "                # ia_temp = get_ia(temp_true, temp_pred)\n",
    "                # u95_temp = get_u95(temp_true, temp_pred)\n",
    "                # mre_temp = get_mre(temp_true, temp_pred)\n",
    "            \n",
    "                # eval_df = pd.concat([eval_df, pd.DataFrame(np.array([[v, t, int(len(frac_true)), r_frac, mae_frac, rmse_frac, ia_frac, u95_frac, mre_frac, r_temp, mae_temp, rmse_temp, ia_temp, u95_temp, mre_temp]]), \n",
    "                #                                         columns=['velo_in', 'temp_in', 'len_data', 'r_frac', 'mae_frac', 'rmse_frac', 'ia_frac', 'u95_frac', 'mre_frac', 'r_temp', 'mae_temp', 'rmse_temp', 'ia_temp', 'u95_temp', 'mre_temp'])])\n",
    "    \n",
    "    # eval_df.to_csv('./results/eval_index_mlp.csv', index=False)\n",
    "    return None\n",
    "\n",
    "result_file = './results/predict_from_csv_result_mlp.csv'\n",
    "plot_result(result_file)\n",
    "# print(get_eval_index(result_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   velo_in  temp_in  len_data    r_frac  mae_frac  rmse_frac   ia_frac  \\\n",
      "0      0.7     85.0      47.0  0.999934  0.003481   0.004732  0.999951   \n",
      "0      0.7     90.0      44.0  0.999851  0.009553   0.010375  0.999764   \n",
      "0      0.7     95.0      41.0  0.999780  0.014736   0.015965  0.999434   \n",
      "\n",
      "   u95_frac      mre_frac    r_temp  mae_temp  rmse_temp   ia_temp  u95_temp  \\\n",
      "0  0.013114  1.422088e-01  0.999891  0.247596   0.297436  0.999845  0.704593   \n",
      "0  0.025046  4.527244e+26  0.999754  0.315501   0.344122  0.999831  0.938072   \n",
      "0  0.037823  1.103187e+28  0.999590  0.375693   0.428525  0.999774  1.180711   \n",
      "\n",
      "   mre_temp  \n",
      "0  0.004906  \n",
      "0  0.005681  \n",
      "0  0.006444  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.eval import get_ia, get_mae, get_mse, get_rmse, get_r, get_u95, get_mre\n",
    "def get_eval_index(result_file):\n",
    "    result_df = pd.read_csv(result_file)\n",
    "    eval_df = pd.DataFrame(columns=['velo_in', 'temp_in', 'len_data', 'r_frac', 'mae_frac', 'rmse_frac', 'ia_frac', 'u95_frac', 'mre_frac', 'r_temp', 'mae_temp', 'rmse_temp', 'ia_temp', 'u95_temp', 'mre_temp'])\n",
    "    \n",
    "    velo_in_config = [0.7, 0.8, 0.9, 1.0]\n",
    "    temp_in_config = [80.0, 85.0, 90.0, 95.0]\n",
    "\n",
    "    for v in velo_in_config:\n",
    "        for t in temp_in_config:\n",
    "            frac_true = []\n",
    "            frac_pred = []\n",
    "            temp_true = []\n",
    "            temp_pred = []\n",
    "            for i in range(len(result_df)):\n",
    "                if result_df['velo_in'][i] == v and result_df['temp_in'][i] == t:\n",
    "                    frac_true.append(result_df['frac_true'][i])\n",
    "                    frac_pred.append(result_df['frac_pred'][i])\n",
    "                    temp_true.append(result_df['temp_pcm_true'][i])\n",
    "                    temp_pred.append(result_df['temp_pcm_pred'][i])\n",
    "            \n",
    "            if not len(frac_true) == 0:\n",
    "            \n",
    "                r_frac = get_r(frac_true, frac_pred)\n",
    "                mae_frac = get_mae(frac_true, frac_pred)\n",
    "                rmse_frac = get_rmse(frac_true, frac_pred)\n",
    "                ia_frac = get_ia(frac_true, frac_pred)\n",
    "                u95_frac = get_u95(frac_true, frac_pred)\n",
    "                mre_frac = get_mre(frac_true, frac_pred)\n",
    "                r_temp = get_r(temp_true, temp_pred)\n",
    "                mae_temp = get_mae(temp_true, temp_pred)\n",
    "                rmse_temp = get_rmse(temp_true, temp_pred)\n",
    "                ia_temp = get_ia(temp_true, temp_pred)\n",
    "                u95_temp = get_u95(temp_true, temp_pred)\n",
    "                mre_temp = get_mre(temp_true, temp_pred)\n",
    "\n",
    "                # print(pd.DataFrame(np.array([[v, t, len(frac_true), r_frac, mae_frac, rmse_frac, ia_frac, u95_frac, r_temp, mae_temp, rmse_temp, ia_temp, u95_temp]]), \n",
    "                #                     columns=['velo_in', 'temp_in', 'len_data', 'r_frac', 'mae_frac', 'rmse_frac', 'ia_frac', 'u95_frac', 'r_temp', 'mae_temp', 'rmse_temp', 'ia_temp', 'u95_temp']))\n",
    "            \n",
    "                eval_df = pd.concat([eval_df, pd.DataFrame(np.array([[v, t, int(len(frac_true)), r_frac, mae_frac, rmse_frac, ia_frac, u95_frac, mre_frac, r_temp, mae_temp, rmse_temp, ia_temp, u95_temp, mre_temp]]), \n",
    "                                                        columns=['velo_in', 'temp_in', 'len_data', 'r_frac', 'mae_frac', 'rmse_frac', 'ia_frac', 'u95_frac', 'mre_frac', 'r_temp', 'mae_temp', 'rmse_temp', 'ia_temp', 'u95_temp', 'mre_temp'])])\n",
    "    \n",
    "    eval_df.to_csv('./results/eval_index_mlp.csv', index=False)\n",
    "    return eval_df\n",
    "\n",
    "result_file = './results/predict_from_csv_result_mlp.csv'                \n",
    "print(get_eval_index(result_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict_seq(velo_in: float, temp_in: float, time_range: int, time_step: int) -> np:\n",
    "    # model.load_weights(\"lstm.hdf5\")\n",
    "    time_arr = np.array(range(0, time_range, time_step))\n",
    "    velo_in_arr = np.array([velo_in] * time_arr.shape[0])\n",
    "    temp_in_arr = np.array([temp_in] * time_arr.shape[0])\n",
    "    input_arr = np.stack((velo_in_arr, temp_in_arr, time_arr), axis=1)\n",
    "    input_arr_copy = input_arr.copy()\n",
    "    input_arr = scaler_x.transform(input_arr)\n",
    "    # input_arr = scaler_x.transform(input_arr).reshape(-1, 1, 3)\n",
    "    pred_result = model.predict(input_arr)\n",
    "    pred_result = scaler_y.inverse_transform(pred_result)\n",
    "    # print(pred_resilt)\n",
    "    # print(input.shape)\n",
    "    # print(input)\n",
    "    # print(pred_result)\n",
    "    # print(pred_result.shape)\n",
    "    frac_pred = pred_result[:, 0]\n",
    "    frac_pred[frac_pred > 1.0] = 1.0  # 11\n",
    "    frac_pred[frac_pred < 0.0] = 0.0\n",
    "\n",
    "    temp_pcm_pred = pred_result[:, 1]\n",
    "    for i in range(input_arr.shape[0]):\n",
    "        temp_pcm_pred[i] = np.min((temp_pcm_pred[i], input_arr_copy[i, 1]))\n",
    "\n",
    "    pred_df = pd.DataFrame({\"time\": time_arr, \"frac_pred\": frac_pred, \"temp_pred\":temp_pcm_pred})\n",
    "    pred_df.to_csv(f\"./results/predict/velo_{velo_in}_temp_{temp_in}_result.csv\", index=False)\n",
    "    return None\n",
    "\n",
    "\n",
    "# velo_in_config = [0.7, 0.8, 0.9, 1.0, 2.0]\n",
    "# temp_in_config = [90.0,]\n",
    "velo_in_config = [0.7, 0.8, 0.9, 1.0, 2.0]\n",
    "temp_in_config = [80.0, 85.0, 90.0, 95.0, 100.0]\n",
    "\n",
    "for v in velo_in_config:\n",
    "    for t in temp_in_config:\n",
    "        model_predict_seq(velo_in = v, temp_in = t, time_range = 1500, time_step = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('luliu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ebb4956837553ce6594ce7068d255f516acc14f8c6a53ff4de21536caf393ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
